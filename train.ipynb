
import os
import sqlite3
import pandas as pd
import numpy as np
import requests
import joblib
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
import base64

# Set paths
DATA_DB = 'joined_data.db'
MODELS_DIR = 'models_v1'
os.makedirs(MODELS_DIR, exist_ok=True)

# GitHub credentials and repo details
GITHUB_TOKEN = "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"
GITHUB_REPO = "chiragpalan/test"
BRANCH = "main"
GITHUB_FOLDER = "models_v1"

def download_database():
    url = 'https://raw.githubusercontent.com/chiragpalan/final_project/main/database/joined_data.db'
    response = requests.get(url)
    if response.status_code == 200:
        with open(DATA_DB, 'wb') as f:
            f.write(response.content)
        print("Database downloaded successfully.")
    else:
        raise Exception("Failed to download the database.")

def get_table_names(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [row[0] for row in cursor.fetchall()]
    conn.close()
    return tables

def load_data_from_table(db_path, table_name):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    conn.close()
    return df

def check_and_clean_data(X):
    if not np.isfinite(X.values).all():
        print("Warning: X contains NaN, infinity, or very large values. Cleaning data...")
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
    return X

def train_random_forest(X_train, y_train, table_name):
    model = RandomForestRegressor(random_state=42)
    param_distributions = {
        'n_estimators': [100, 200, 400, 800],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }

    search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=3, random_state=42, n_jobs=-1)
    search.fit(X_train, y_train)
    best_model = search.best_estimator_
    save_model(best_model, table_name, 'random_forest')

def train_gradient_boosting(X_train, y_train, table_name):
    model = GradientBoostingRegressor(random_state=42)
    param_distributions = {
        'n_estimators': [100, 200, 400, 800],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7, 9],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=3, random_state=42, n_jobs=-1)
    search.fit(X_train, y_train)
    best_model = search.best_estimator_
    save_model(best_model, table_name, 'gradient_boosting')

def train_xgboost(X_train, y_train, table_name):
    model = XGBRegressor(random_state=42)
    param_distributions = {
        'n_estimators': [100, 200, 400, 800],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7, 9],
        'min_child_weight': [1, 2, 4],
        'subsample': [0.6, 0.8, 1.0]
    }

    search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=3, random_state=42, n_jobs=-1)
    search.fit(X_train, y_train)
    best_model = search.best_estimator_
    save_model(best_model, table_name, 'xgboost')

def save_model(model, table_name, model_type):
    filename = f"{MODELS_DIR}/{table_name}_{model_type}.joblib"
    joblib.dump(model, filename)
    print(f"Saved model: {filename}")
    upload_to_github(filename, GITHUB_TOKEN, GITHUB_REPO, BRANCH, GITHUB_FOLDER)

def upload_to_github(file_path, github_token, repo, branch, folder):
    filename = os.path.basename(file_path)
    url = f"https://api.github.com/repos/{repo}/contents/{folder}/{filename}"

    with open(file_path, "rb") as f:
        content = f.read()

    content_base64 = base64.b64encode(content).decode("utf-8")

    headers = {
        "Authorization": f"token {github_token}",
        "Accept": "application/vnd.github+json",
    }
    response = requests.get(url, headers=headers)

    data = {
        "message": f"Add {filename}",
        "content": content_base64,
        "branch": branch
    }

    if response.status_code == 200:
        sha = response.json()["sha"]
        data["sha"] = sha
        print(f"Updating existing file: {filename}")
    elif response.status_code == 404:
        print(f"Creating new file: {filename}")
    else:
        print(f"Failed to check file existence. Status code: {response.status_code}")
        print(response.json())
        return

    response = requests.put(url, headers=headers, json=data)
    if response.status_code in [200, 201]:
        print(f"Successfully uploaded {filename} to GitHub.")
    else:
        print(f"Failed to upload {filename}. Status code: {response.status_code}")
        print(response.json())

def main():
    download_database()
    tables = get_table_names(DATA_DB)

    for table in tables:
        print(f"Processing table: {table}")
        df = load_data_from_table(DATA_DB, table).dropna()

        if 'date' in df.index.names:
            df.reset_index(inplace=True)

        X = df.drop(columns=['Date', 'target_n7d'], errors='ignore')
        y = df['target_n7d']

        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, shuffle=True)
        scaler = StandardScaler()

        X_train_cleaned = check_and_clean_data(X_train)
        X_train_scaled = scaler.fit_transform(X_train_cleaned)

        train_random_forest(X_train_scaled, y_train, table)
        train_gradient_boosting(X_train_scaled, y_train, table)
        train_xgboost(X_train_scaled, y_train, table)

if __name__ == "__main__":
    main()
